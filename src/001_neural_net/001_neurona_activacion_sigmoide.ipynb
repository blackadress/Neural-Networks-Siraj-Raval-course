{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "Estos son los datos que estaremos usando para *entrenar* nuestra neurona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_entrenamiento_x = np.array([\n",
    "        [0, 0, 1],\n",
    "        [1, 1, 1],\n",
    "        [1, 0, 1],\n",
    "        [0, 1, 1]\n",
    "    ])\n",
    "data_entrenamiento_y = np.array([\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de activación sigmoide\n",
    "\n",
    "La función de activación sigmoide es la que medirá el error y aproximará los *pesos* de la neurona, la sigmoide es una función en forma de *S*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoide(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí definimos la derivada de la sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def derivada_sigmoide(x):\n",
    "    return 1 * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciando Numeros aleatorios determinísticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "#sinapsis\n",
    "\n",
    "sinapsis0 = 2*np.random.random((3,4)) - 1\n",
    "sinapsis1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "#la función \"random\" únicamente nos da valores tipo float que van desde 0 a 1\n",
    "#entonces para tener valores negativos y positivos que van de -1 a 1, \n",
    "#multiplicamos por 2 y restamos 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error \n",
      " [[0.47372957]\n",
      " [0.45529163]\n",
      " [0.45615914]\n",
      " [0.48895696]]\n",
      "Error \n",
      " [[9.03104406e-07]\n",
      " [1.13411539e-01]\n",
      " [6.34943668e-05]\n",
      " [1.54256257e-02]]\n",
      "Error \n",
      " [[7.75877198e-08]\n",
      " [7.91574829e-02]\n",
      " [9.75413692e-06]\n",
      " [7.45550467e-03]]\n",
      "Error \n",
      " [[1.89540807e-08]\n",
      " [6.41551478e-02]\n",
      " [3.33748284e-06]\n",
      " [4.88555260e-03]]\n",
      "Error \n",
      " [[7.04405642e-09]\n",
      " [5.52941759e-02]\n",
      " [1.57460755e-06]\n",
      " [3.62515253e-03]]\n",
      "Error \n",
      " [[3.28547314e-09]\n",
      " [4.92881991e-02]\n",
      " [8.83722822e-07]\n",
      " [2.87860928e-03]]\n",
      "Error \n",
      " [[1.76709087e-09]\n",
      " [4.48778193e-02]\n",
      " [5.52922810e-07]\n",
      " [2.38556517e-03]]\n",
      "Error \n",
      " [[1.04800408e-09]\n",
      " [4.14642220e-02]\n",
      " [3.72679753e-07]\n",
      " [2.03592427e-03]]\n",
      "Error \n",
      " [[6.67394168e-10]\n",
      " [3.87218823e-02]\n",
      " [2.65166517e-07]\n",
      " [1.77520238e-03]]\n",
      "Error \n",
      " [[4.48664197e-10]\n",
      " [3.64567697e-02]\n",
      " [1.96592671e-07]\n",
      " [1.57337881e-03]]\n",
      "Predicciones después de entrenamiento\n",
      "[[3.14846268e-10]\n",
      " [9.65453079e-01]\n",
      " [9.99999849e-01]\n",
      " [1.41270748e-03]]\n"
     ]
    }
   ],
   "source": [
    "#definimos la cantidad de iteraciones en las que entrenará nuestra red neuronal\n",
    "iteraciones = 10000\n",
    "\n",
    "for i in range(iteraciones):\n",
    "    #La capa_0 corresponde a los datos con los que alimentamos a nuestra red neuronal\n",
    "    capa_0 = data_entrenamiento_x\n",
    "    \n",
    "    #La capa_1 corresponde a la \"operación sigmoide aplicada a cada 'peso'(sinapsis0) multiplicado por \n",
    "    #cada valor de la capa_0(capa de input de la red neuronal)\"\n",
    "    #notese que no estoy usando \"sesgos\" para cada una de las conexiones, ningun sesgo de hecho\n",
    "    capa_1 = sigmoide(np.dot(capa_0, sinapsis0))\n",
    "    \n",
    "    #consecuentemente la capa_2 corresponde a la \"operación sigmoide aplicada a peso * capa_1\"\n",
    "    #algo así capa_2[i] = sigmoide(sinapsis1[i] x capa_1[i])\n",
    "    #se usa la multiplicación punto entre vectores para simplificar el proceso descrito en la linea anterior\n",
    "    capa_2 = sigmoide(np.dot(capa_1, sinapsis1))\n",
    "    \n",
    "    #Ahora el error, definido por: (error = valor_real - valor_obtenido_por_el_modelo)\n",
    "    capa_2_error = data_entrenamiento_y - capa_2\n",
    "    \n",
    "    #EL delta es la multiplicación de el error x derivada de  la sigmoide\n",
    "    capa_2_delta = capa_2_error * derivada_sigmoide(capa_2)\n",
    "    \n",
    "    #Hacemos lo mismo para la capa 1 solo que con diferentes valores, usaremos los valores de la sinapsis 1 como\n",
    "    #nuestro output y lo multiplicamos por el delta de la capa 2\n",
    "    capa_1_error = np.dot(capa_2_delta, sinapsis1.T) #<---\n",
    "    capa_1_delta = capa_1_error * derivada_sigmoide(capa_1)\n",
    "    \n",
    "    #Las derivadas de sigmoide que encontramos son en realidad los vectores gradiente, estos vectores \n",
    "    #son los que nos indican dirección(hacia donde está creciendo la función), al multiplicarlos por el error de\n",
    "    #cada capa, lo que estamos haciendo es dar \"pasos\" con dirección para minimizar el error de nuestras funciones\n",
    "    \n",
    "    sinapsis1 += np.dot(capa_1.T, capa_2_delta)\n",
    "    sinapsis0 += np.dot(capa_0.T, capa_1_delta)\n",
    "    \n",
    "    #Por razones de debugging y también para ver qué tanto va cambiando nuestra red en el tiempo, \n",
    "    #imprimiremos los errores cada 1000 iteraciones.\n",
    "    if i % 1000 == 0:\n",
    "        print(\"Error \\n %s\" %np.abs(capa_2_error))\n",
    "\n",
    "print(\"Predicciones después de entrenamiento\")\n",
    "print(capa_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones\n",
    "El output real es **[[0],[1],[1],[0]]** Nuestra red demuestra acercarce bastante a estos valores:\n",
    "+ 3.14846268e-10 = 0.0000000003...~ 0\n",
    "+ 9.65453079e-01 = 0.965 ~ 1\n",
    "+ 9.99999849e-01 = 0.999 ~ 1\n",
    "+ 1.41270748e-03 = 0.001... ~ 1\n",
    "\n",
    "Recursos de matemáticas:\n",
    "\n",
    "+Derivada[Explicación español](https://www.youtube.com/watch?v=ia8L26ub_pc) - [3Blue1Brown - Lista de reproducción](https://www.youtube.com/watch?v=ia8L26ub_pc)\n",
    "\n",
    "+Vector\n",
    "\n",
    "+Gradiente / gradiente descendente [khan-academy 1](https://es.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient) [khan-academy 1](https://es.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient-and-graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
